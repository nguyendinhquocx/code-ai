# Data Analytics Workflow - Phases 0-4

**File nÃ y**: Phases 0-4 (Setup â†’ Business Analysis)
**LiÃªn káº¿t**: [`workflow-analytics-tong-quan.md`](workflow-analytics-tong-quan.md)


---

## PHASE 0: Setup & Understanding

### Step 0.1: Read User Input
**Má»¥c Ä‘Ã­ch**: Hiá»ƒu requirements vÃ  context

**Actions**:
1. User kÃ©o file vÃ o hoáº·c cung cáº¥p path
2. User giáº£i thÃ­ch vÃ i cÃ¢u vá» dá»¯ liá»‡u
3. Agent há»i lÃ m rÃµ náº¿u cáº§n

**Questions Agent NÃªn Há»i**:
```
1. Má»¥c Ä‘Ã­ch phÃ¢n tÃ­ch?
   - So sÃ¡nh periods (2025 vs 2026)?
   - TÃ¬m growth drivers?
   - Identify risks?
   - Other?

2. Output mong muá»‘n?
   - Charts only?
   - Insights + charts?
   - Full analysis package?

3. Business context?
   - Financial? Marketing? Operations?
   - Key metrics quan tÃ¢m?
   - Thresholds/targets?
```

**Decision Point**:
```
IF user unclear â†’ Ask 2-3 clarifying questions MAX
IF user very clear â†’ Proceed to Phase 1
```

### Step 0.2: Create Project Structure
**Actions**:
```bash
mkdir -p [project_name]/{code,charts,document,statics/{code,charts,document}}
```

**Logging**:
```
âœ… Created project structure at: [path]
```

---

## PHASE 1: Data Ingestion

### Step 1.1: Load Raw Data

**Supported Formats**:
- Excel: `.xlsx`, `.xls`
- CSV: `.csv`
- JSON: `.json`
- PDF: `.pdf` (extract tables)

**Code Template**:
```python
import pandas as pd

def load_data(file_path, file_type='auto'):
    """
    Load data with auto-detection
    """
    if file_type == 'auto':
        file_type = file_path.split('.')[-1].lower()

    if file_type in ['xlsx', 'xls']:
        # Excel: Load all sheets
        dfs = pd.read_excel(file_path, sheet_name=None)
        print(f"âœ… Loaded {len(dfs)} sheets: {list(dfs.keys())}")
        return dfs

    elif file_type == 'csv':
        df = pd.read_csv(file_path)
        print(f"âœ… Loaded CSV: {df.shape}")
        return {'data': df}

    elif file_type == 'json':
        df = pd.read_json(file_path)
        print(f"âœ… Loaded JSON: {df.shape}")
        return {'data': df}

    else:
        raise ValueError(f"Unsupported file type: {file_type}")
```

**Validation**:
```python
# Check if loaded successfully
assert dfs is not None, "âŒ Failed to load data"
assert len(dfs) > 0, "âŒ No data found"

# Log summary
for sheet_name, df in dfs.items():
    print(f"  {sheet_name}: {df.shape[0]} rows Ã— {df.shape[1]} cols")
```

**Decision Point**:
```
IF Excel with multiple sheets:
  â†’ Ask user: "Which sheet(s) to analyze?"
  OR auto-detect main data sheet (largest one)

IF CSV/JSON single file:
  â†’ Proceed

IF load fails:
  â†’ Try different encoding (utf-8, latin1, cp1252)
  â†’ If still fails â†’ Ask user
```

### Step 1.2: Initial Data Inspection

**Actions**:
```python
# For each dataframe
for name, df in dfs.items():
    print(f"\n=== {name} ===")
    print(f"Shape: {df.shape}")
    print(f"Columns: {list(df.columns)}")
    print(f"\nFirst 3 rows:\n{df.head(3)}")
    print(f"\nData types:\n{df.dtypes}")
```

**Logging**:
```
âœ… Inspected data: [sheet_name]
   - Rows: N
   - Columns: M
   - Preview: OK
```

---

## PHASE 2: Data Quality Check

### Step 2.1: Schema Validation

**Check Structure**:
```python
def validate_schema(df, expected_cols=None):
    """
    Validate data structure
    """
    issues = []

    # Check if empty
    if df.empty:
        issues.append("âŒ DataFrame is empty")

    # Check columns
    if expected_cols:
        missing = set(expected_cols) - set(df.columns)
        if missing:
            issues.append(f"âŒ Missing columns: {missing}")

    # Check duplicates
    dup_count = df.duplicated().sum()
    if dup_count > 0:
        issues.append(f"âš ï¸ {dup_count} duplicate rows found")

    return issues
```

**Decision Point**:
```
IF critical issues (empty, missing key columns):
  â†’ STOP â†’ Ask user

IF warnings only (duplicates, unexpected columns):
  â†’ LOG warning â†’ Continue
```

### Step 2.2: Data Quality Metrics

**Calculate**:
```python
def data_quality_report(df):
    """
    Generate data quality report
    """
    report = {
        'total_rows': len(df),
        'total_cols': len(df.columns),
        'missing_values': df.isnull().sum().to_dict(),
        'missing_pct': (df.isnull().sum() / len(df) * 100).round(2).to_dict(),
        'duplicates': df.duplicated().sum(),
        'dtypes': df.dtypes.to_dict()
    }

    # Flag issues
    report['critical_issues'] = []
    report['warnings'] = []

    for col, pct in report['missing_pct'].items():
        if pct > 50:
            report['critical_issues'].append(f"{col}: {pct}% missing")
        elif pct > 10:
            report['warnings'].append(f"{col}: {pct}% missing")

    return report
```

**Validation Gate**:
```
IF critical_issues:
  â†’ STOP
  â†’ Report to user
  â†’ Ask: "Fix data or proceed with caution?"

IF warnings only:
  â†’ LOG warnings
  â†’ Proceed
```

### Step 2.3: Data Cleaning

**Auto-Fix (Safe operations)**:
```python
def auto_clean(df, revenue_cols):
    """
    Safe auto-cleaning
    """
    df_clean = df.copy()

    # 1. Fill missing numeric values with 0 (for revenue columns)
    for col in revenue_cols:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].fillna(0)
            print(f"  âœ… Filled {col} missing values with 0")

    # 2. Remove duplicate rows (if any)
    before = len(df_clean)
    df_clean = df_clean.drop_duplicates()
    after = len(df_clean)
    if before > after:
        print(f"  âœ… Removed {before - after} duplicate rows")

    # 3. Fix data types (if needed)
    # ... type conversions

    return df_clean
```

**Logging**:
```
âœ… Data cleaned
   - Missing values filled: [columns]
   - Duplicates removed: N
   - Type conversions: [if any]
```

---

## PHASE 3: EDA & Descriptive Statistics

**CRITICAL**: Táº¡o STATICS MODULE Äáº¦Y Äá»¦ Ä‘á»ƒ cÃ³ thá»ƒ Ä‘á»c hiá»ƒu, tinh chá»‰nh khi data thay Ä‘á»•i

### Step 3.0: Táº¡o Statics Module Structure

**Má»¥c Ä‘Ã­ch**: Táº¡o code cÃ³ thá»ƒ edit, reuse, maintain khi data thay Ä‘á»•i

**Actions**:
```bash
# Táº¡o folder structure
mkdir -p "[project_name]/statics/code"
mkdir -p "[project_name]/statics/document"
mkdir -p "[project_name]/statics/charts"
# hoáº·c charts_eda tuá»³ project
```

**Output**: Cáº¥u trÃºc sáºµn sÃ ng cho EDA code

### Step 3.1: Táº¡o EDA Code Files (EDITABLE)

**QUAN TRá»ŒNG**: Táº¡o Cáº¢ 2 versions - notebook (interactive) vÃ  script (automated)

**File 1: `statics/code/eda.ipynb`** (Interactive Jupyter Notebook)

**Sections báº¯t buá»™c**:
1. Setup & Imports
2. Load Data
3. Data Quality Check
4. Clean Data
5. Descriptive Statistics (mean, median, std, quartiles)
6. Distribution Analysis (skewness, kurtosis)
7. Outlier Detection (IQR method)
8. Coefficient of Variation (CV) - QUAN TRá»ŒNG
9. Correlation Analysis
10. Time Series Analysis
11. Export Summary Statistics
12. Key Findings Summary

**Template Code**:
```python
# Cell 1: Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from scipy import stats

# Design settings
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 10
COLORS = {
    'primary': '#000000',
    'secondary': '#4A4A4A',
    'tertiary': '#808080',
    'light': '#B0B0B0'
}

# Cell 2: Load Data
BASE_DIR = Path.cwd().parent.parent.parent.parent.parent
DATA_FILE = BASE_DIR / "path/to/data.xlsx"
df = pd.read_excel(DATA_FILE)

# Cell 3-11: [Full EDA analysis sections...]

# Cell 12: Export
stats_summary.to_csv(OUTPUT_DIR / "eda_statistics.csv", index=False)
print("âœ… EDA completed")
```

**File 2: `statics/code/generate_charts.py`** (Automated Chart Generation)

**Má»¥c Ä‘Ã­ch**: Generate 7 standard EDA charts tá»± Ä‘á»™ng

**7 Charts báº¯t buá»™c**:
1. `01_boxplot_revenue_by_year.png` - Outliers & distribution
2. `02_histogram_revenue_distribution.png` - Overall distribution
3. `03_correlation_heatmap.png` - Variable relationships
4. `04_violin_revenue_by_year.png` - Distribution shape
5. `05_timeseries_monthly_trend.png` - Time patterns
6. `06_cv_comparison_by_year.png` - Variability analysis
7. `07_statistics_summary_table.png` - Stats table as image

**Template Code**:
```python
"""
Generate 7 Standard EDA Charts
Following WORKFLOW_ANALYTICS.md design principles
"""
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

# Setup
BASE_DIR = Path(__file__).parent.parent.parent.parent.parent
DATA_FILE = BASE_DIR / "path/to/data.xlsx"
CHARTS_DIR = Path(__file__).parent.parent / "charts_eda"
CHARTS_DIR.mkdir(exist_ok=True)

# Load & Clean
df = pd.read_excel(DATA_FILE)
df['doanh thu'] = df['doanh thu'].fillna(0)

# Chart 1: Boxplot
fig, ax = plt.subplots(figsize=(10, 6))
# ... [plotting code]
plt.savefig(CHARTS_DIR / "01_boxplot_revenue_by_year.png", dpi=300)

# ... [Charts 2-7]

print("âœ… All 7 EDA charts generated")
```

**File 3: `statics/README.md`** (Documentation)

**Ná»™i dung báº¯t buá»™c**:
- Purpose cá»§a module
- Quick start guide (how to run notebook vÃ  script)
- List of charts generated
- Key statistics explained
- Customization options
- Integration vá»›i business analysis
- Troubleshooting tips

**KHÃ”NG DÃ™NG EMOJI, KHÃ”NG ICON** (theo AGENTS.md)

### Step 3.2: Run EDA Analysis

**Option A: Interactive** (khi cáº§n explore vÃ  tinh chá»‰nh)
```bash
cd "[project_name]/statics/code"
jupyter notebook eda.ipynb
# Execute all cells manually
```

**Option B: Automated** (khi data structure Ä‘Ã£ clear)
```bash
cd "[project_name]/statics/code"
python generate_charts.py
```

**Validation**:
```python
# Check outputs
import os

expected_charts = [
    '01_boxplot_revenue_by_year.png',
    '02_histogram_revenue_distribution.png',
    '03_correlation_heatmap.png',
    '04_violin_revenue_by_year.png',
    '05_timeseries_monthly_trend.png',
    '06_cv_comparison_by_year.png',
    '07_statistics_summary_table.png'
]

charts_dir = 'statics/charts_eda'  # or 'statics/charts'
missing = []
for chart in expected_charts:
    path = f'{charts_dir}/{chart}'
    if not os.path.exists(path):
        missing.append(chart)

if missing:
    print(f"âŒ Missing charts: {missing}")
else:
    print("âœ… All 7 EDA charts generated")

# Check stats CSV
stats_file = 'statics/document/eda_statistics.csv'
if os.path.exists(stats_file):
    print(f"âœ… Stats CSV exists: {stats_file}")
else:
    print(f"âš ï¸  Stats CSV not found: {stats_file}")
```

### Step 3.3: Analyze EDA Results

**Key Metrics to Extract**:
```python
# From stats CSV
stats_2025 = pd.read_csv('statics/document/csv/stats_2025.csv', index_col=0)
stats_2026 = pd.read_csv('statics/document/csv/stats_2026.csv', index_col=0)
cv = pd.read_csv('statics/document/csv/coefficient_of_variation.csv', index_col=0)

# Identify patterns
high_cv_channels = cv[cv['CV 2025 (%)'] > 50].index.tolist()
stable_channels = cv[cv['CV 2025 (%)'] < 20].index.tolist()

print(f"High CV (>50%): {high_cv_channels}")
print(f"Stable (<20%): {stable_channels}")
```

**Decision Tree**:
```
FOR each channel:
  IF CV > 50%:
    â†’ Mark as "High Risk"
    â†’ Forecast method: Use MEDIAN + separate peak months
    â†’ Buffer: 30%+

  ELIF CV 20-50%:
    â†’ Mark as "Medium Risk"
    â†’ Forecast method: MEAN with monthly monitoring
    â†’ Buffer: 15-20%

  ELSE (CV < 20%):
    â†’ Mark as "Stable"
    â†’ Forecast method: MEAN
    â†’ Buffer: 5-10%
```

**Store Decisions**:
```python
# Save for later use in insights
eda_insights = {
    'high_risk_channels': high_cv_channels,
    'stable_channels': stable_channels,
    'outlier_months': {},  # From boxplots
    'correlations': {},     # From heatmap
    'seasonality': {}       # From time series
}

# Export
import json
with open('statics/document/eda_insights.json', 'w') as f:
    json.dump(eda_insights, f, indent=2)
```

---

## PHASE 4: Business Analysis

**CRITICAL**: Táº¡o CODE EDITABLE cho business metrics Ä‘á»ƒ cÃ³ thá»ƒ tinh chá»‰nh khi yÃªu cáº§u thay Ä‘á»•i

### Step 4.0: Táº¡o Business Analysis Code Files

**QUAN TRá»ŒNG**: Táº¡o Cáº¢ 2 versions - notebook (interactive) vÃ  script (automated)

**File 1: `code/analysis.ipynb`** (Interactive Jupyter Notebook)

**Má»¥c Ä‘Ã­ch**:
- TÃ­nh toÃ¡n business metrics cÃ³ thá»ƒ chá»‰nh sá»­a
- Interactive Ä‘á»ƒ test scenarios khÃ¡c nhau
- Dá»… dÃ ng thay Ä‘á»•i assumptions vÃ  re-run

**Sections báº¯t buá»™c**:
1. Setup (imports, paths)
2. Load & Clean Data
3. Data Quality Check
4. Extract specific data (vd: Báº£o Viá»‡t, top companies)
5. Calculate Key Metrics (revenue by year, percentages, etc.)
6. Calculate Growth Rates
7. Scenario Analysis (multiple scenarios: accept, refuse, negotiate)
8. Cost Analysis
9. Summary Statistics
10. Top N Analysis (companies, channels, etc.)
11. Save Metrics (JSON, CSV)
12. Key Findings & Recommendation

**Template Code**:
```python
# Cell 1: Setup
import pandas as pd
import numpy as np
import json
from pathlib import Path

BASE_DIR = Path.cwd().parent
DATA_FILE = BASE_DIR.parent.parent.parent / "path/to/data.xlsx"
OUTPUT_DIR = BASE_DIR / "document"
OUTPUT_DIR.mkdir(exist_ok=True)

# Cell 2-11: [Business analysis sections...]

# Cell 12: Summary
print("="*60)
print("ANALYSIS COMPLETE")
print("="*60)
print(f"\nKey finding: [...]")
print(f"\nRecommendation: [...]")
```

**File 2: `code/analysis.py`** (Script Version)

**Má»¥c Ä‘Ã­ch**:
- Automated execution
- Reproducible analysis
- Can be scheduled or run in CI/CD

**Template Code**:
```python
"""
Business Analysis Script
TÃ­nh toÃ¡n metrics vÃ  generate insights
"""
import pandas as pd
import numpy as np
import json
from pathlib import Path

# =============================================================================
# SETUP
# =============================================================================

BASE_DIR = Path(__file__).parent.parent
DATA_FILE = BASE_DIR.parent.parent.parent / "path/to/data.xlsx"
OUTPUT_DIR = BASE_DIR / "document"
OUTPUT_DIR.mkdir(exist_ok=True)

print("=== BUSINESS ANALYSIS ===\n")

# =============================================================================
# 1. LOAD & CLEAN DATA
# =============================================================================

print("1. Loading data...")
df = pd.read_excel(DATA_FILE)
df['doanh thu'] = df['doanh thu'].fillna(0)
print(f"   Loaded: {len(df)} records")

# =============================================================================
# 2-10. [Business analysis sections...]
# =============================================================================

# ... [all metrics calculations]

# =============================================================================
# 11. SAVE METRICS
# =============================================================================

print("\n11. Saving metrics...")
output_file = OUTPUT_DIR / "metrics.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(metrics, f, indent=2, ensure_ascii=False)
print(f"   âœ… Saved: {output_file}")

# =============================================================================
# 12. SUMMARY
# =============================================================================

print("\n" + "="*60)
print("âœ… ANALYSIS COMPLETE")
print("="*60)
print(f"\nKey finding: [...]")
print(f"\nRecommendation: [...]")
print("="*60)
```

**LÆ°u Ã½**:
- Code pháº£i cÃ³ comment tiáº¿ng Viá»‡t rÃµ rÃ ng
- Section headers rÃµ rÃ ng vá»›i `# =============`
- Print progress Ä‘á»ƒ user biáº¿t Ä‘ang á»Ÿ Ä‘Ã¢u
- Táº¥t cáº£ paths dÃ¹ng `Path()` object (cross-platform)

### Step 4.1: Calculate Business Metrics

**Common Metrics** (adjust based on context):

```python
def calculate_business_metrics(df_2025, df_2026, revenue_cols):
    """
    Calculate key business metrics
    """
    metrics = {}

    # 1. Growth Rate
    metrics['growth_rate'] = {}
    for col in revenue_cols:
        growth = ((df_2026[col].sum() / df_2025[col].sum()) - 1) * 100
        metrics['growth_rate'][col] = round(growth, 2)

    # 2. Growth Contribution (KEY METRIC!)
    df_contrib = pd.DataFrame({'Thang': df_2026['thang']})
    for col in revenue_cols:
        delta_kenh = df_2026[col].values - df_2025[col].values
        delta_tong = df_2026['TONG'].values - df_2025['TONG'].values
        df_contrib[col] = (delta_kenh / delta_tong * 100).round(1)

    metrics['growth_contribution'] = df_contrib[revenue_cols].mean().to_dict()

    # 3. Market Share (tá»‰ trá»ng)
    metrics['market_share_2025'] = (df_2025[revenue_cols].sum() / df_2025['TONG'].sum() * 100).round(2).to_dict()
    metrics['market_share_2026'] = (df_2026[revenue_cols].sum() / df_2026['TONG'].sum() * 100).round(2).to_dict()

    # 4. Peak Months (tá»« EDA)
    metrics['peak_months'] = {}
    for col in revenue_cols:
        peak_month = df_2026[col].idxmax() + 1  # +1 vÃ¬ index tá»« 0
        metrics['peak_months'][col] = int(df_2026.loc[df_2026[col].idxmax(), 'thang'])

    return metrics
```

**Store Results**:
```python
# Save metrics
with open('document/business_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=2)

print("âœ… Business metrics calculated and saved")
```

### Step 4.2: Identify Key Insights

**Template**:
```python
def extract_insights(metrics, eda_insights):
    """
    Extract actionable insights
    """
    insights = []

    # 1. Growth Drivers
    growth_contrib = metrics['growth_contribution']
    top_driver = max(growth_contrib, key=growth_contrib.get)
    insights.append({
        'type': 'growth_driver',
        'title': f"{top_driver} lÃ  Ä‘á»™ng cÆ¡ tÄƒng trÆ°á»Ÿng chÃ­nh",
        'data': f"{growth_contrib[top_driver]:.1f}% Ä‘Ã³ng gÃ³p",
        'action': f"Focus resources vÃ o {top_driver}"
    })

    # 2. Risk Channels (tá»« EDA)
    for channel in eda_insights['high_risk_channels']:
        insights.append({
            'type': 'risk',
            'title': f"{channel} biáº¿n Ä‘á»™ng cao (CV > 50%)",
            'data': f"KhÃ³ dá»± Ä‘oÃ¡n, cáº§n buffer lá»›n",
            'action': f"Forecast riÃªng tá»«ng thÃ¡ng cho {channel}, buffer 30%+"
        })

    # 3. Seasonality
    for channel, peak_month in metrics['peak_months'].items():
        if peak_month in [1, 6, 7, 8, 9]:  # Known seasonal months
            insights.append({
                'type': 'seasonality',
                'title': f"{channel} Ä‘á»‰nh thÃ¡ng {peak_month}",
                'data': f"Cáº§n chuáº©n bá»‹ trÆ°á»›c 2-3 thÃ¡ng",
                'action': f"Reserve budget & resources tá»« thÃ¡ng {peak_month - 2}"
            })

    # 4. Cross-sell Opportunities (tá»« correlation)
    # ... (náº¿u cÃ³ correlation cao)

    return insights
```

---

## ğŸ”— LIÃŠN Káº¾T Vá»šI CÃC FILE KHÃC

- **Tá»•ng quan**: [`workflow-analytics-tong-quan.md`](workflow-analytics-tong-quan.md)
- **Phases 5-8**: [`workflow-analytics-phases-5-8.md`](workflow-analytics-phases-5-8.md)
- **Error Handling**: [`workflow-analytics-error-handling.md`](workflow-analytics-error-handling.md)
- **Decision Trees**: [`workflow-analytics-decision-trees.md`](workflow-analytics-decision-trees.md)
- **Configuration**: [`workflow-analytics-configuration.md`](workflow-analytics-configuration.md)
- **Examples**: [`workflow-analytics-examples.md`](workflow-analytics-examples.md)