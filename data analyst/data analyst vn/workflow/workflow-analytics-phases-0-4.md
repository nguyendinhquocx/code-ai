# Data Analytics Workflow - Phases 0-4

**File nÃ y**: Phases 0-4 (Setup â†’ Business Analysis)
**LiÃªn káº¿t**: [`workflow-analytics-tong-quan.md`](workflow-analytics-tong-quan.md) | [`phases-5-8.md`](workflow-analytics-phases-5-8.md)

---

## ğŸ“‘ Quick Navigation

| Phase | Ná»™i dung | Khi nÃ o dÃ¹ng |
|-------|----------|--------------|
| [Phase 0](#phase-0-setup--understanding) | Setup & Question Framing | Báº¯t Ä‘áº§u project má»›i, peek data, gá»£i Ã½ phÃ¢n tÃ­ch |
| [Phase 1](#phase-1-data-ingestion) | Data Ingestion | Load Excel/CSV, inspect schema |
| [Phase 2](#phase-2-data-quality-check) | Data Quality Check | Validate, clean, handle missing/outliers |
| [Phase 3](#phase-3-eda-exploratory-data-analysis) | EDA (Statistical Charts) | Boxplot, histogram, correlation, CV analysis |
| [Phase 4](#phase-4-business-analysis) | Business Analysis | Calculate metrics, growth, segmentation |

**Next**: [Phases 5-8 (Visualization â†’ Delivery)](workflow-analytics-phases-5-8.md)

---

## PHASE 0: Setup & Understanding

### Step 0.1: Quick Data Peek (TRÆ¯á»šC KHI Há»I)
**Má»¥c Ä‘Ã­ch**: Äá»c nhanh file Ä‘á»ƒ hiá»ƒu cáº¥u trÃºc â†’ Gá»¢I Ã cÃ¢u há»i thÃ´ng minh

**Actions**:
```python
# Load NHANH Ä‘á»ƒ peek (khÃ´ng phÃ¢n tÃ­ch sÃ¢u)
import pandas as pd

# Load all sheets
dfs = pd.read_excel(file_path, sheet_name=None)

# Quick inspection
data_summary = {}
for sheet_name, df in dfs.items():
    data_summary[sheet_name] = {
        'rows': len(df),
        'columns': list(df.columns),
        'date_cols': [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()],
        'numeric_cols': df.select_dtypes(include=['number']).columns.tolist(),
        'categorical_cols': df.select_dtypes(include=['object']).columns.tolist(),
        'sample_data': df.head(2).to_dict()
    }

print(f"âœ… Peeked {len(dfs)} sheets: {list(dfs.keys())}")
```

**Logging**:
```
âœ… Quick peek completed
   Sheet 'data': 1122 rows, 15 columns
   - Date columns: ['NgÃ y', 'ThÃ¡ng']
   - Numeric: ['Doanh thu', 'Sá»‘ lÆ°á»£ng', ...]
   - Categorical: ['KÃªnh', 'Sáº£n pháº©m', 'Khu vá»±c', ...]
```

### Step 0.2: Smart Question Framing
**Má»¥c Ä‘Ã­ch**: Dá»±a vÃ o peek â†’ Gá»¢I Ã phÃ¢n tÃ­ch phÃ¹ há»£p

**AI Agent Pháº£i LÃ€M**:
1. **PhÃ¢n tÃ­ch cáº¥u trÃºc** tá»« data_summary
2. **Gá»£i Ã½ 3-5 cÃ¢u há»i phÃ¢n tÃ­ch** phÃ¹ há»£p vá»›i data
3. **Há»i user chá»n** HOáº¶C user tá»± Ä‘á» xuáº¥t

**Template Gá»£i Ã vá»›i Recommendation**:
```
ğŸ“Š Tao Ä‘Ã£ Ä‘á»c qua file data cá»§a mÃ y. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c phÃ¢n tÃ­ch tao cÃ³ thá»ƒ lÃ m:

[Dá»±a vÃ o data structure, AI analyze vÃ  recommend:]

ğŸ’¡ **KHUYáº¾N NGHá»Š: Option [X] - Comprehensive Analysis**
   LÃ½ do: [Giáº£i thÃ­ch dá»±a trÃªn data structure]
   - VD: "Data cÃ³ {rows} rows, time series tá»« {start_year}-{end_year},
     {n_channels} kÃªnh khÃ¡c nhau â†’ nÃªn phÃ¢n tÃ­ch Ä‘áº§y Ä‘á»§ Ä‘á»ƒ hiá»ƒu toÃ n cáº£nh"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. **PhÃ¢n tÃ­ch xu hÆ°á»›ng theo thá»i gian** â­ (vÃ¬ cÃ³ cá»™t 'NgÃ y', 'ThÃ¡ng')
   - So sÃ¡nh doanh thu 2023 vs 2024 vs 2025
   - TÃ¬m thÃ¡ng peak/low performance
   - Xu hÆ°á»›ng tÄƒng/giáº£m

2. **PhÃ¢n tÃ­ch theo kÃªnh/sáº£n pháº©m** â­ (vÃ¬ cÃ³ cá»™t 'KÃªnh', 'Sáº£n pháº©m')
   - KÃªnh nÃ o Ä‘Ã³ng gÃ³p nhiá»u nháº¥t?
   - Sáº£n pháº©m nÃ o growth/decline máº¡nh?
   - So sÃ¡nh performance giá»¯a cÃ¡c kÃªnh

3. **PhÃ¢n tÃ­ch outliers & cháº¥t lÆ°á»£ng dá»¯ liá»‡u** â­
   - ThÃ¡ng nÃ o cÃ³ doanh thu báº¥t thÆ°á»ng?
   - Missing data á»Ÿ Ä‘Ã¢u?
   - Äá»™ á»•n Ä‘á»‹nh (CV) cá»§a tá»«ng kÃªnh

4. **PhÃ¢n tÃ­ch Ä‘á»‘i tÃ¡c cá»¥ thá»ƒ** (náº¿u cÃ³ cá»™t partner name)
   - VD: Báº£o Viá»‡t chiáº¿m % bao nhiÃªu?
   - Xu hÆ°á»›ng tÄƒng/giáº£m cá»§a Ä‘á»‘i tÃ¡c X?

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MÃ y muá»‘n:
  (1) LÃ m theo khuyáº¿n nghá»‹
  (2) Chá»n riÃªng (VD: "1,2,3" hoáº·c "chá»‰ lÃ m 1")
  (3) MÃ´ táº£ ngáº¯n
```

**Logic Recommendation** (Agent tá»± Ä‘á»™ng analyze):
```python
# Recommendation logic based on data characteristics
recommendation_score = {}

# Score each option based on data structure
if len(data_summary['date_cols']) > 0 and data_summary['rows'] > 50:
    recommendation_score['time_series'] = 10  # High priority

if len(data_summary['categorical_cols']) >= 2:
    recommendation_score['segmentation'] = 10  # High priority

if data_summary['rows'] > 100:
    recommendation_score['statistical'] = 8  # Medium-high priority

# Specific partner analysis if partner columns detected
partner_cols = [col for col in df.columns if any(x in col.lower() for x in ['partner', 'kÃªnh', 'channel', 'Ä‘á»‘i tÃ¡c', 'báº£o viá»‡t'])]
if partner_cols:
    recommendation_score['partner'] = 9

# ALWAYS include quality check
recommendation_score['quality'] = 10

# Determine recommendation
total_high_priority = sum(1 for score in recommendation_score.values() if score >= 9)

if total_high_priority >= 3:
    recommendation = "Comprehensive (lÃ m háº¿t options cÃ³ â­)"
    reasoning = f"Data cÃ³ Ä‘á»§ Ä‘iá»u kiá»‡n cho phÃ¢n tÃ­ch toÃ n diá»‡n: {rows} rows, time series {years}, {n_categories} categories"
elif total_high_priority >= 2:
    top_options = [k for k, v in sorted(recommendation_score.items(), key=lambda x: -x[1])[:2]]
    recommendation = f"Focus on {top_options}"
    reasoning = "Æ¯u tiÃªn 2 phÃ¢n tÃ­ch quan trá»ng nháº¥t dá»±a trÃªn cáº¥u trÃºc data"
else:
    top_option = max(recommendation_score.items(), key=lambda x: x[1])[0]
    recommendation = f"Option {top_option}"
    reasoning = "Data structure phÃ¹ há»£p nháº¥t vá»›i phÃ¢n tÃ­ch nÃ y"

# Output recommendation prominently
print(f"ğŸ’¡ KHUYáº¾N NGHá»Š: {recommendation}")
print(f"   LÃ½ do: {reasoning}")
```

**Quy táº¯c Gá»£i Ã**:
```python
# Logic gá»£i Ã½ tá»± Ä‘á»™ng
suggestions = []

# 1. CÃ³ cá»™t date/time â†’ Suggest time series
if data_summary['date_cols']:
    suggestions.append("Time series analysis (xu hÆ°á»›ng theo thá»i gian)")

# 2. CÃ³ >1 categorical column â†’ Suggest segmentation
if len(data_summary['categorical_cols']) > 0:
    cats = ', '.join(data_summary['categorical_cols'][:3])
    suggestions.append(f"Segmentation analysis (phÃ¢n tÃ­ch theo {cats})")

# 3. CÃ³ numeric columns â†’ Suggest statistical analysis
if len(data_summary['numeric_cols']) > 2:
    suggestions.append("Statistical analysis (outliers, distribution, correlation)")

# 4. CÃ³ partner/channel columns â†’ Suggest partner analysis
partner_cols = [col for col in df.columns if any(x in col.lower() for x in ['partner', 'kÃªnh', 'channel', 'Ä‘á»‘i tÃ¡c'])]
if partner_cols:
    suggestions.append(f"Partner/Channel contribution analysis")

# 5. ALWAYS suggest: Data quality check
suggestions.append("Data quality check (missing, duplicates, anomalies)")
```

**User Response Handling**:
```
IF user reply "1" hoáº·c "yes" hoáº·c "ok" hoáº·c "lÃ m theo khuyáº¿n nghá»‹":
  â†’ Proceed vá»›i recommendation (thÆ°á»ng lÃ  comprehensive)
  â†’ Confirm: "OK, tao sáº½ lÃ m [recommendation]"

IF user chá»n specific options (VD: "2,3" hoáº·c "chá»‰ lÃ m 1"):
  â†’ Parse selections â†’ Proceed vá»›i chá»‰ nhá»¯ng options Ä‘Ã³
  â†’ Confirm: "OK, tao sáº½ focus vÃ o [selected options]"

IF user tá»± mÃ´ táº£ (VD: "phÃ¢n tÃ­ch xu hÆ°á»›ng Báº£o Viá»‡t"):
  â†’ Parse mÃ´ táº£ â†’ map to analysis type
  â†’ Confirm vá»›i user: "Tao hiá»ƒu lÃ  mÃ y muá»‘n [X], Ä‘Ãºng khÃ´ng?"
  â†’ Wait for confirmation

IF user nÃ³i "lÃ m háº¿t" hoáº·c "full analysis" hoáº·c "all":
  â†’ Comprehensive (táº¥t cáº£ suggestions)

IF user unclear hoáº·c vague:
  â†’ Default to recommendation
  â†’ Inform: "Tao sáº½ lÃ m theo khuyáº¿n nghá»‹: [recommendation]"
```

**Smart Defaults** (Ä‘á»ƒ workflow trÆ¡n tru):
```
LUÃ”N Æ¯U TIÃŠN: User chá»‰ cáº§n 1 tá»« Ä‘á»ƒ proceed
- "yes" / "1" / "ok" â†’ LÃ m theo recommendation
- "2" / "1,2,3" â†’ LÃ m specific options
- "all" / "háº¿t" â†’ LÃ m táº¥t cáº£

KHÃ”NG YÃŠU Cáº¦U: User viáº¿t cÃ¢u dÃ i
â†’ Giáº£m friction, tÄƒng tá»‘c Ä‘á»™ workflow
```

**Questions Agent CÃ“ THá»‚ Há»i ThÃªm** (Tá»I ÄA 2-3 cÃ¢u):
```
1. Metric chÃ­nh mÃ y quan tÃ¢m nháº¥t? (revenue, quantity, margin, etc.)
2. CÃ³ target/threshold nÃ o cáº§n so sÃ¡nh khÃ´ng? (VD: target 100M/thÃ¡ng)
3. Output: Chá»‰ charts, hay charts + insights + recommendations?
```

**Decision Point**:
```
IF user response RÃ• RÃ€NG:
  â†’ Proceed to Phase 1

IF user still unclear:
  â†’ Default to "Comprehensive analysis" (lÃ m háº¿t)
  â†’ Proceed to Phase 1
```

### Step 0.3: Create Project Structure
**Actions**:
```bash
mkdir -p [project_name]/{code,charts,document,statics/{code,charts,document}}
```

**Logging**:
```
âœ… Created project structure at: [path]
```

---

**SUMMARY PHASE 0**:
```
Quy trÃ¬nh:
1. Äá»c nhanh file (peek) â†’ Hiá»ƒu cáº¥u trÃºc
2. Gá»£i Ã½ 3-5 phÃ¢n tÃ­ch phÃ¹ há»£p dá»±a vÃ o data
3. User chá»n hoáº·c mÃ´ táº£ â†’ Confirm
4. Táº¡o project structure â†’ Proceed

Lá»£i Ã­ch:
âœ“ AI chá»§ Ä‘á»™ng gá»£i Ã½ (khÃ´ng há»i mÃ¹ quÃ¡ng)
âœ“ User hiá»ƒu rÃµ options
âœ“ Workflow trÆ¡n tru, khÃ´ng rÆ°á»m rÃ 
âœ“ Tá»‘i Ä‘a 2-3 cÃ¢u há»i clarifying
```

---

## PHASE 1: Data Ingestion

### Step 1.1: Load Raw Data

**Supported Formats**:
- Excel: `.xlsx`, `.xls`
- CSV: `.csv`
- JSON: `.json`
- PDF: `.pdf` (extract tables)

**Code Template**:
```python
import pandas as pd

def load_data(file_path, file_type='auto'):
    """
    Load data with auto-detection
    """
    if file_type == 'auto':
        file_type = file_path.split('.')[-1].lower()

    if file_type in ['xlsx', 'xls']:
        # Excel: Load all sheets
        dfs = pd.read_excel(file_path, sheet_name=None)
        print(f"âœ… Loaded {len(dfs)} sheets: {list(dfs.keys())}")
        return dfs

    elif file_type == 'csv':
        df = pd.read_csv(file_path)
        print(f"âœ… Loaded CSV: {df.shape}")
        return {'data': df}

    elif file_type == 'json':
        df = pd.read_json(file_path)
        print(f"âœ… Loaded JSON: {df.shape}")
        return {'data': df}

    else:
        raise ValueError(f"Unsupported file type: {file_type}")
```

**Validation**:
```python
# Check if loaded successfully
assert dfs is not None, "âŒ Failed to load data"
assert len(dfs) > 0, "âŒ No data found"

# Log summary
for sheet_name, df in dfs.items():
    print(f"  {sheet_name}: {df.shape[0]} rows Ã— {df.shape[1]} cols")
```

**Decision Point**:
```
IF Excel with multiple sheets:
  â†’ Ask user: "Which sheet(s) to analyze?"
  OR auto-detect main data sheet (largest one)

IF CSV/JSON single file:
  â†’ Proceed

IF load fails:
  â†’ Try different encoding (utf-8, latin1, cp1252)
  â†’ If still fails â†’ Ask user
```

### Step 1.2: Initial Data Inspection

**Actions**:
```python
# For each dataframe
for name, df in dfs.items():
    print(f"\n=== {name} ===")
    print(f"Shape: {df.shape}")
    print(f"Columns: {list(df.columns)}")
    print(f"\nFirst 3 rows:\n{df.head(3)}")
    print(f"\nData types:\n{df.dtypes}")
```

**Logging**:
```
âœ… Inspected data: [sheet_name]
   - Rows: N
   - Columns: M
   - Preview: OK
```

---

## PHASE 2: Data Quality Check

### Step 2.1: Schema Validation

**Check Structure**:
```python
def validate_schema(df, expected_cols=None):
    """
    Validate data structure
    """
    issues = []

    # Check if empty
    if df.empty:
        issues.append("âŒ DataFrame is empty")

    # Check columns
    if expected_cols:
        missing = set(expected_cols) - set(df.columns)
        if missing:
            issues.append(f"âŒ Missing columns: {missing}")

    # Check duplicates
    dup_count = df.duplicated().sum()
    if dup_count > 0:
        issues.append(f"âš ï¸ {dup_count} duplicate rows found")

    return issues
```

**Decision Point**:
```
IF critical issues (empty, missing key columns):
  â†’ STOP â†’ Ask user

IF warnings only (duplicates, unexpected columns):
  â†’ LOG warning â†’ Continue
```

### Step 2.2: Data Quality Metrics

**Calculate**:
```python
def data_quality_report(df):
    """
    Generate data quality report
    """
    report = {
        'total_rows': len(df),
        'total_cols': len(df.columns),
        'missing_values': df.isnull().sum().to_dict(),
        'missing_pct': (df.isnull().sum() / len(df) * 100).round(2).to_dict(),
        'duplicates': df.duplicated().sum(),
        'dtypes': df.dtypes.to_dict()
    }

    # Flag issues
    report['critical_issues'] = []
    report['warnings'] = []

    for col, pct in report['missing_pct'].items():
        if pct > 50:
            report['critical_issues'].append(f"{col}: {pct}% missing")
        elif pct > 10:
            report['warnings'].append(f"{col}: {pct}% missing")

    return report
```

**Validation Gate**:
```
IF critical_issues:
  â†’ STOP
  â†’ Report to user
  â†’ Ask: "Fix data or proceed with caution?"

IF warnings only:
  â†’ LOG warnings
  â†’ Proceed
```

### Step 2.3: Data Cleaning

**Auto-Fix (Safe operations)**:
```python
def auto_clean(df, revenue_cols):
    """
    Safe auto-cleaning
    """
    df_clean = df.copy()

    # 1. Fill missing numeric values with 0 (for revenue columns)
    for col in revenue_cols:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].fillna(0)
            print(f"  âœ… Filled {col} missing values with 0")

    # 2. Remove duplicate rows (if any)
    before = len(df_clean)
    df_clean = df_clean.drop_duplicates()
    after = len(df_clean)
    if before > after:
        print(f"  âœ… Removed {before - after} duplicate rows")

    # 3. Fix data types (if needed)
    # ... type conversions

    return df_clean
```

**Logging**:
```
âœ… Data cleaned
   - Missing values filled: [columns]
   - Duplicates removed: N
   - Type conversions: [if any]
```

---

## PHASE 3: EDA & Descriptive Statistics

**CRITICAL**: Táº¡o STATICS MODULE Äáº¦Y Äá»¦ Ä‘á»ƒ cÃ³ thá»ƒ Ä‘á»c hiá»ƒu, tinh chá»‰nh khi data thay Ä‘á»•i

**AGENT AUTONOMY - Linh hoáº¡t & SÃ¡ng táº¡o:**
```
NguyÃªn táº¯c thiáº¿t káº¿ lÃ  HÆ¯á»šNG DáºªN, khÃ´ng pháº£i LUáº¬T Cá»¨NG.

Agent Ä‘Æ°á»£c phÃ©p:
âœ“ Tá»± do Ä‘iá»u chá»‰nh mÃ u sáº¯c, layout, chart types dá»±a trÃªn data context
âœ“ PhÃ¡ vá»¡ quy táº¯c náº¿u cÃ³ lÃ½ do chÃ­nh Ä‘Ã¡ng (VD: data outliers cá»±c Ä‘oan)
âœ“ Chá»n chart type phÃ¹ há»£p nháº¥t (boxplot, violin, kde, etc.)
âœ“ Thá»­ nghiá»‡m vÃ  tá»‘i Æ°u Ä‘á»ƒ patterns/anomalies ná»•i báº­t

Quy trÃ¬nh tÆ° duy:
1. Äá»c nguyÃªn táº¯c thiáº¿t káº¿ â†’ Hiá»ƒu TINH THáº¦N, khÃ´ng pháº£i tá»«ng chá»¯
2. PhÃ¢n tÃ­ch data distribution â†’ Quyáº¿t Ä‘á»‹nh style & chart type
3. Táº¡o chart â†’ Ãp dá»¥ng nguyÃªn táº¯c LINH HOáº T
4. Tá»± há»i: "Chart nÃ y reveal insights gÃ¬? CÃ³ cáº§n simplify khÃ´ng?"
5. Optimize dá»±a trÃªn cÃ¢u tráº£ lá»i

Má»¤C TIÃŠU: Insights ná»•i báº­t, khÃ´ng pháº£i follow rules mÃ¹ quÃ¡ng.
```

### Step 3.0: Táº¡o Statics Module Structure

**Má»¥c Ä‘Ã­ch**: Táº¡o code cÃ³ thá»ƒ edit, reuse, maintain khi data thay Ä‘á»•i

**Actions**:
```bash
# Táº¡o folder structure
mkdir -p "[project_name]/statics/code"
mkdir -p "[project_name]/statics/document"
mkdir -p "[project_name]/statics/charts"
# hoáº·c charts_eda tuá»³ project
```

**Output**: Cáº¥u trÃºc sáºµn sÃ ng cho EDA code

### Step 3.1: Táº¡o EDA Code Files (EDITABLE)

**QUAN TRá»ŒNG**: Táº¡o Cáº¢ 2 versions - notebook (interactive) vÃ  script (automated)

**File 1: `statics/code/eda.ipynb`** (Interactive Jupyter Notebook)

**Sections báº¯t buá»™c**:
1. Setup & Imports
2. Load Data
3. Data Quality Check
4. Clean Data
5. Descriptive Statistics (mean, median, std, quartiles)
6. Distribution Analysis (skewness, kurtosis)
7. Outlier Detection (IQR method)
8. Coefficient of Variation (CV) - QUAN TRá»ŒNG
9. Correlation Analysis
10. Time Series Analysis
11. Export Summary Statistics
12. Key Findings Summary

**Template Code**:
```python
# Cell 1: Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from scipy import stats

# Design settings - Aesthetic & Professional
plt.rcParams['figure.dpi'] = 300
plt.rcParams['font.size'] = 10
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['axes.linewidth'] = 1.2
plt.rcParams['grid.linewidth'] = 0.8

# Color palette: Muted, elegant tones (inspired by modern data viz)
# Base: Soft blue-grays for professional look
# Accent: Muted pastels for data points
COLORS = {
    'primary': '#2C3E50',      # Dark blue-gray (main text, borders)
    'secondary': '#546E7A',    # Medium blue-gray (secondary elements)
    'accent': '#7986CB',       # Soft indigo (highlights, important data)
    'success': '#81C784',      # Muted green (positive trends)
    'warning': '#FFB74D',      # Soft orange (warnings, attention)
    'danger': '#E57373',       # Muted red (negative trends, outliers)
    'neutral': '#B0BEC5',      # Light gray (backgrounds, grids)
    'background': '#FAFAFA'    # Off-white (chart backgrounds)
}

# Chart-specific palettes
PALETTE_SEQUENTIAL = ['#E8EAF6', '#C5CAE9', '#9FA8DA', '#7986CB', '#5C6BC0', '#3F51B5']  # Light to dark blue
PALETTE_DIVERGING = ['#E57373', '#FFCC80', '#FFF9C4', '#C5E1A5', '#81C784', '#66BB6A']  # Red to green
PALETTE_CATEGORICAL = ['#7986CB', '#81C784', '#FFB74D', '#E57373', '#64B5F6', '#A1887F']  # Distinct muted colors

# =============================================================================
# AESTHETIC DEFAULTS - Modern Minimalist (EDA Charts)
# =============================================================================
# LÆ¯U Ã: EDA charts thÆ°á»ng phá»©c táº¡p hÆ¡n business charts (nhiá»u data points)
#        â†’ ÄÆ°á»£c phÃ©p dÃ¹ng mÃ u sáº¯c cÃ³ Ã½ nghÄ©a (outliers=red, etc.)
#        â†’ NHÆ¯NG váº«n Æ°u tiÃªn máº£nh, tinh táº¿, minimal
#
# TRIáº¾T LÃ:
# - Tá»‘i giáº£n, hiá»‡n Ä‘áº¡i (kiá»ƒu shadcn/Vercel/React)
# - MÃ u sáº¯c cÃ³ Ã½ nghÄ©a (red=outliers, green=positive, NOT random colors)
# - Máº£nh, thanh thoÃ¡t (line 0.5-2px, marker 6-8px, border 0.5-1px)
# - Grid subtle khi cáº§n (alpha 0.1-0.2, KHÃ”NG 0.3+)
#
# MÃ€U Sáº®C CHO EDA:
# - Boxplot: Muted blue (#7986CB), outliers muted red (#E57373)
# - Histogram: Muted colors vá»›i thin dark edges
# - Correlation: Diverging palette (muted red to muted green)
# - Line charts: Black/dark (#000000, #2C3E50), width 1.5-2px
# - Violin: Soft pastels vá»›i thin edges (0.5-1px)
#
# DECORATION CHO EDA:
# - Grid: Alpha 0.1-0.2 (nháº¹, subtle) hoáº·c OFF cho simple charts
# - Spines: #E0E0E0, width 0.5-1px (máº£nh)
# - Line width: 0.5-2px (KHÃ”NG 3px+, quÃ¡ thÃ´)
# - Marker size: 6-8px (KHÃ”NG 10px+, quÃ¡ to)

# CRITICAL: Chart Design Philosophy - "Less is More"
# =====================================================
# NGUYÃŠN Táº®C Ná»€N Táº¢NG: Biá»ƒu Ä‘á»“ pháº£i thá»ƒ hiá»‡n INSIGHTS, khÃ´ng pháº£i decoration
#
# "Remove everything that doesn't add value. Then remove more."
# - Edward Tufte, Data Visualization Pioneer
#
# A. MINIMALISM BY DATA COMPLEXITY
# ---------------------------------
# Quy táº¯c: Ãt data points â†’ Ãt decoration
#
# 1-2 data points (HIáº¾M trong EDA):
#    â†’ TUYá»†T Äá»I Tá»I GIáº¢N
#    â†’ KHÃ”NG grid, KHÃ”NG viá»n thá»«a
#    â†’ MÃ u: White/black/light gray ONLY
#    â†’ NÃ©t máº£nh (0.5-2px), elegant
#
# 3-5 data points:
#    â†’ Minimal decoration
#    â†’ Grid OFF (trá»« khi thá»±c sá»± cáº§n Ä‘á»c giÃ¡ trá»‹)
#    â†’ MÃ u: Muted tones, Ã­t mÃ u (2-3 colors max)
#    â†’ Line width: 1-2px
#
# 6+ data points (PHá»” BIáº¾N trong EDA - boxplot, histogram, time series):
#    â†’ Decoration CÃ“ THá»‚ nhÆ°ng subtle
#    â†’ Grid alpha 0.1-0.2 (KHÃ”NG dÃ¹ng 0.3+, quÃ¡ Ä‘áº­m)
#    â†’ MÃ u: Palette cÃ³ Ã½ nghÄ©a (red=negative/outliers, green=positive)
#    â†’ Line width: 1-2px (KHÃ”NG 3px+)
#    â†’ Border: 0.5-1px (máº£nh)
#
# B. COLOR HIERARCHY - MÃ u pháº£i cÃ³ má»¥c Ä‘Ã­ch
# ------------------------------------------
# Quy táº¯c: "Color attracts attention. Use it wisely."
#
# Data chÃ­nh (focus):
#    â†’ Muted accent colors: #7986CB (soft indigo), #81C784 (muted green)
#    â†’ KHÃ”NG dÃ¹ng bright/saturated colors (quÃ¡ loÃ¨)
#
# Outliers/Warnings:
#    â†’ Muted red (#E57373) - KHÃ”NG pure red
#
# Positive trends:
#    â†’ Muted green (#81C784) - KHÃ”NG bright green
#
# Background elements (grid, borders, spines):
#    â†’ Ráº¤T nháº¡t (#E0E0E0, #B0BEC5)
#    â†’ Alpha tháº¥p (0.1-0.2)
#    â†’ Width máº£nh (0.5-1px)
#
# C. CRITICAL: Chart Scale Best Practices
# ----------------------------------------
# 1. PERCENTAGE CHARTS: ALWAYS 0-100% scale
#    - Biá»ƒu Ä‘á»“ % PHáº¢I cháº¡y tá»« 0-100%, KHÃ”NG BAO GIá»œ crop
#    - VÃ­ dá»¥: CV 27% â†’ y-axis 0-100%, KHÃ”NG 0-30%
#    - LÃ½ do: TrÃ¡nh misleading visualization
#
# 2. ABSOLUTE VALUES: Start from 0 (unless negative values exist)
#    - Revenue, counts â†’ start from 0
#    - Growth rates (cÃ³ thá»ƒ Ã¢m) â†’ include 0 trong range
#
# 3. Y-AXIS BUFFER: ThÃªm 10-15% padding
#    - ax.set_ylim(0, max_value * 1.15)
#    - Äá»ƒ value labels khÃ´ng bá»‹ crop
#
# 4. GRIDLINES: Subtle, KHÃ”NG dominant
#    - Alpha: 0.1-0.2 (KHÃ”NG dÃ¹ng 0.3+, quÃ¡ Ä‘áº­m)
#    - Linestyle: '--' hoáº·c ':'
#    - Width: 0.5-0.8px (máº£nh)
#    - 1-2 points: OFF
#    - 3-5 points: OFF (trá»« khi cáº§n)
#    - 6+ points: Subtle (alpha 0.1-0.2)
#
# D. VISUAL HIERARCHY - Thá»© tá»± Æ°u tiÃªn
# -------------------------------------
# 1. DATA (bars, lines, points) - Äáº­m nháº¥t, rÃµ nháº¥t
# 2. VALUE LABELS - Gáº§n data, clear
# 3. AXES LABELS & TITLE - Medium weight
# 4. GRID & BORDERS - Ráº¤T nháº¡t, alpha 0.1-0.2
# 5. BACKGROUND - White hoáº·c off-white (#FAFAFA)
#
# E. MODERN AESTHETIC - Thin & Elegant
# -------------------------------------
# - Line width: 0.5-2px (KHÃ”NG 3px+, quÃ¡ thÃ´)
# - Marker size: 6-8px (KHÃ”NG 10px+, quÃ¡ to)
# - Borders: 0.5-1px (thanh thoÃ¡t)
# - Grid alpha: 0.1-0.2 (KHÃ”NG 0.3+, quÃ¡ Ä‘áº­m)
# - Spines: Light gray (#E0E0E0), thin (0.5-1px)
# - Colors: Muted tones (soft pastels, KHÃ”NG bright/saturated)
#
# F. EXAMPLES - Anti-patterns vs Good patterns
# ---------------------------------------------
# âŒ BAD: Boxplot + bright blue + thick borders 2px + grid alpha 0.5
# âœ… GOOD: Boxplot + muted blue (#7986CB) + thin borders 0.8px + grid alpha 0.15
#
# âŒ BAD: Histogram + rainbow colors + thick edges 2px + busy background
# âœ… GOOD: Histogram + muted green (#81C784) + thin edges 0.8px + white background
#
# âŒ BAD: Line chart width 3.5px + marker 12px + grid alpha 0.4
# âœ… GOOD: Line chart width 1.5-2px + marker 6-8px + grid alpha 0.15 or OFF
#
# âŒ BAD: Percentage cropped 0-30% + misleading
# âœ… GOOD: Percentage 0-100% + clear context

# Cell 2: Load Data
BASE_DIR = Path.cwd().parent.parent.parent.parent.parent
DATA_FILE = BASE_DIR / "path/to/data.xlsx"
df = pd.read_excel(DATA_FILE)

# Cell 3-11: [Full EDA analysis sections...]

# Cell 12: Export
stats_summary.to_csv(OUTPUT_DIR / "eda_statistics.csv", index=False)
print("âœ… EDA completed")
```

**File 2: `statics/code/generate_charts.py`** (Automated Chart Generation)

**Má»¥c Ä‘Ã­ch**: Generate 7 standard EDA charts tá»± Ä‘á»™ng

**7 Charts báº¯t buá»™c**:
1. `01_boxplot_revenue_by_year.png` - Outliers & distribution
2. `02_histogram_revenue_distribution.png` - Overall distribution
3. `03_correlation_heatmap.png` - Variable relationships
4. `04_violin_revenue_by_year.png` - Distribution shape
5. `05_timeseries_monthly_trend.png` - Time patterns
6. `06_cv_comparison_by_year.png` - Variability analysis
7. `07_statistics_summary_table.png` - Stats table as image

**Template Code**:
```python
"""
Generate 7 Standard EDA Charts
Following WORKFLOW_ANALYTICS.md design principles
"""
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

# Setup
BASE_DIR = Path(__file__).parent.parent.parent.parent.parent
DATA_FILE = BASE_DIR / "path/to/data.xlsx"
CHARTS_DIR = Path(__file__).parent.parent / "charts_eda"
CHARTS_DIR.mkdir(exist_ok=True)

# Load & Clean
df = pd.read_excel(DATA_FILE)
df['doanh thu'] = df['doanh thu'].fillna(0)

# Chart 1: Boxplot
fig, ax = plt.subplots(figsize=(10, 6))
# ... [plotting code]
plt.savefig(CHARTS_DIR / "01_boxplot_revenue_by_year.png", dpi=300)

# ... [Charts 2-7]

print("âœ… All 7 EDA charts generated")
```

**File 3: `statics/README.md`** (Documentation)

**Ná»™i dung báº¯t buá»™c**:
- Purpose cá»§a module
- Quick start guide (how to run notebook vÃ  script)
- List of charts generated
- Key statistics explained
- Customization options
- Integration vá»›i business analysis
- Troubleshooting tips

**KHÃ”NG DÃ™NG EMOJI, KHÃ”NG ICON** (theo AGENTS.md)

### Step 3.2: Run EDA Analysis

**Option A: Interactive** (khi cáº§n explore vÃ  tinh chá»‰nh)
```bash
cd "[project_name]/statics/code"
jupyter notebook eda.ipynb
# Execute all cells manually
```

**Option B: Automated** (khi data structure Ä‘Ã£ clear)
```bash
cd "[project_name]/statics/code"
python generate_charts.py
```

**Validation**:
```python
# Check outputs
import os

expected_charts = [
    '01_boxplot_revenue_by_year.png',
    '02_histogram_revenue_distribution.png',
    '03_correlation_heatmap.png',
    '04_violin_revenue_by_year.png',
    '05_timeseries_monthly_trend.png',
    '06_cv_comparison_by_year.png',
    '07_statistics_summary_table.png'
]

charts_dir = 'statics/charts_eda'  # or 'statics/charts'
missing = []
for chart in expected_charts:
    path = f'{charts_dir}/{chart}'
    if not os.path.exists(path):
        missing.append(chart)

if missing:
    print(f"âŒ Missing charts: {missing}")
else:
    print("âœ… All 7 EDA charts generated")

# Check stats CSV
stats_file = 'statics/document/eda_statistics.csv'
if os.path.exists(stats_file):
    print(f"âœ… Stats CSV exists: {stats_file}")
else:
    print(f"âš ï¸  Stats CSV not found: {stats_file}")
```

### Step 3.3: Analyze EDA Results

**Key Metrics to Extract**:
```python
# From stats CSV
stats_2025 = pd.read_csv('statics/document/csv/stats_2025.csv', index_col=0)
stats_2026 = pd.read_csv('statics/document/csv/stats_2026.csv', index_col=0)
cv = pd.read_csv('statics/document/csv/coefficient_of_variation.csv', index_col=0)

# Identify patterns
high_cv_channels = cv[cv['CV 2025 (%)'] > 50].index.tolist()
stable_channels = cv[cv['CV 2025 (%)'] < 20].index.tolist()

print(f"High CV (>50%): {high_cv_channels}")
print(f"Stable (<20%): {stable_channels}")
```

**Decision Tree**:
```
FOR each channel:
  IF CV > 50%:
    â†’ Mark as "High Risk"
    â†’ Forecast method: Use MEDIAN + separate peak months
    â†’ Buffer: 30%+

  ELIF CV 20-50%:
    â†’ Mark as "Medium Risk"
    â†’ Forecast method: MEAN with monthly monitoring
    â†’ Buffer: 15-20%

  ELSE (CV < 20%):
    â†’ Mark as "Stable"
    â†’ Forecast method: MEAN
    â†’ Buffer: 5-10%
```

**Store Decisions**:
```python
# Save for later use in insights
eda_insights = {
    'high_risk_channels': high_cv_channels,
    'stable_channels': stable_channels,
    'outlier_months': {},  # From boxplots
    'correlations': {},     # From heatmap
    'seasonality': {}       # From time series
}

# Export
import json
with open('statics/document/eda_insights.json', 'w') as f:
    json.dump(eda_insights, f, indent=2)
```

---

## PHASE 4: Business Analysis

**CRITICAL**: Táº¡o CODE EDITABLE cho business metrics Ä‘á»ƒ cÃ³ thá»ƒ tinh chá»‰nh khi yÃªu cáº§u thay Ä‘á»•i

### Step 4.0: Táº¡o Business Analysis Code Files

**QUAN TRá»ŒNG**: Táº¡o Cáº¢ 2 versions - notebook (interactive) vÃ  script (automated)

**File 1: `code/analysis.ipynb`** (Interactive Jupyter Notebook)

**Má»¥c Ä‘Ã­ch**:
- TÃ­nh toÃ¡n business metrics cÃ³ thá»ƒ chá»‰nh sá»­a
- Interactive Ä‘á»ƒ test scenarios khÃ¡c nhau
- Dá»… dÃ ng thay Ä‘á»•i assumptions vÃ  re-run

**Sections báº¯t buá»™c**:
1. Setup (imports, paths)
2. Load & Clean Data
3. Data Quality Check
4. Extract specific data (vd: Báº£o Viá»‡t, top companies)
5. Calculate Key Metrics (revenue by year, percentages, etc.)
6. Calculate Growth Rates
7. Scenario Analysis (multiple scenarios: accept, refuse, negotiate)
8. Cost Analysis
9. Summary Statistics
10. Top N Analysis (companies, channels, etc.)
11. Save Metrics (JSON, CSV)
12. Key Findings & Recommendation

**Template Code**:
```python
# Cell 1: Setup
import pandas as pd
import numpy as np
import json
from pathlib import Path

BASE_DIR = Path.cwd().parent
DATA_FILE = BASE_DIR.parent.parent.parent / "path/to/data.xlsx"
OUTPUT_DIR = BASE_DIR / "document"
OUTPUT_DIR.mkdir(exist_ok=True)

# Cell 2-11: [Business analysis sections...]

# Cell 12: Summary
print("="*60)
print("ANALYSIS COMPLETE")
print("="*60)
print(f"\nKey finding: [...]")
print(f"\nRecommendation: [...]")
```

**File 2: `code/analysis.py`** (Script Version)

**Má»¥c Ä‘Ã­ch**:
- Automated execution
- Reproducible analysis
- Can be scheduled or run in CI/CD

**Template Code**:
```python
"""
Business Analysis Script
TÃ­nh toÃ¡n metrics vÃ  generate insights
"""
import pandas as pd
import numpy as np
import json
from pathlib import Path

# =============================================================================
# SETUP
# =============================================================================

BASE_DIR = Path(__file__).parent.parent
DATA_FILE = BASE_DIR.parent.parent.parent / "path/to/data.xlsx"
OUTPUT_DIR = BASE_DIR / "document"
OUTPUT_DIR.mkdir(exist_ok=True)

print("=== BUSINESS ANALYSIS ===\n")

# =============================================================================
# 1. LOAD & CLEAN DATA
# =============================================================================

print("1. Loading data...")
df = pd.read_excel(DATA_FILE)
df['doanh thu'] = df['doanh thu'].fillna(0)
print(f"   Loaded: {len(df)} records")

# =============================================================================
# 2-10. [Business analysis sections...]
# =============================================================================

# ... [all metrics calculations]

# =============================================================================
# 11. SAVE METRICS
# =============================================================================

print("\n11. Saving metrics...")
output_file = OUTPUT_DIR / "metrics.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(metrics, f, indent=2, ensure_ascii=False)
print(f"   âœ… Saved: {output_file}")

# =============================================================================
# 12. SUMMARY
# =============================================================================

print("\n" + "="*60)
print("âœ… ANALYSIS COMPLETE")
print("="*60)
print(f"\nKey finding: [...]")
print(f"\nRecommendation: [...]")
print("="*60)
```

**LÆ°u Ã½**:
- Code pháº£i cÃ³ comment tiáº¿ng Viá»‡t rÃµ rÃ ng
- Section headers rÃµ rÃ ng vá»›i `# =============`
- Print progress Ä‘á»ƒ user biáº¿t Ä‘ang á»Ÿ Ä‘Ã¢u
- Táº¥t cáº£ paths dÃ¹ng `Path()` object (cross-platform)

### Step 4.1: Calculate Business Metrics

**Common Metrics** (adjust based on context):

```python
def calculate_business_metrics(df_2025, df_2026, revenue_cols):
    """
    Calculate key business metrics
    """
    metrics = {}

    # 1. Growth Rate
    metrics['growth_rate'] = {}
    for col in revenue_cols:
        growth = ((df_2026[col].sum() / df_2025[col].sum()) - 1) * 100
        metrics['growth_rate'][col] = round(growth, 2)

    # 2. Growth Contribution (KEY METRIC!)
    df_contrib = pd.DataFrame({'Thang': df_2026['thang']})
    for col in revenue_cols:
        delta_kenh = df_2026[col].values - df_2025[col].values
        delta_tong = df_2026['TONG'].values - df_2025['TONG'].values
        df_contrib[col] = (delta_kenh / delta_tong * 100).round(1)

    metrics['growth_contribution'] = df_contrib[revenue_cols].mean().to_dict()

    # 3. Market Share (tá»‰ trá»ng)
    metrics['market_share_2025'] = (df_2025[revenue_cols].sum() / df_2025['TONG'].sum() * 100).round(2).to_dict()
    metrics['market_share_2026'] = (df_2026[revenue_cols].sum() / df_2026['TONG'].sum() * 100).round(2).to_dict()

    # 4. Peak Months (tá»« EDA)
    metrics['peak_months'] = {}
    for col in revenue_cols:
        peak_month = df_2026[col].idxmax() + 1  # +1 vÃ¬ index tá»« 0
        metrics['peak_months'][col] = int(df_2026.loc[df_2026[col].idxmax(), 'thang'])

    return metrics
```

**Store Results**:
```python
# Save metrics
with open('document/business_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=2)

print("âœ… Business metrics calculated and saved")
```

### Step 4.2: Identify Key Insights

**Template**:
```python
def extract_insights(metrics, eda_insights):
    """
    Extract actionable insights
    """
    insights = []

    # 1. Growth Drivers
    growth_contrib = metrics['growth_contribution']
    top_driver = max(growth_contrib, key=growth_contrib.get)
    insights.append({
        'type': 'growth_driver',
        'title': f"{top_driver} lÃ  Ä‘á»™ng cÆ¡ tÄƒng trÆ°á»Ÿng chÃ­nh",
        'data': f"{growth_contrib[top_driver]:.1f}% Ä‘Ã³ng gÃ³p",
        'action': f"Focus resources vÃ o {top_driver}"
    })

    # 2. Risk Channels (tá»« EDA)
    for channel in eda_insights['high_risk_channels']:
        insights.append({
            'type': 'risk',
            'title': f"{channel} biáº¿n Ä‘á»™ng cao (CV > 50%)",
            'data': f"KhÃ³ dá»± Ä‘oÃ¡n, cáº§n buffer lá»›n",
            'action': f"Forecast riÃªng tá»«ng thÃ¡ng cho {channel}, buffer 30%+"
        })

    # 3. Seasonality
    for channel, peak_month in metrics['peak_months'].items():
        if peak_month in [1, 6, 7, 8, 9]:  # Known seasonal months
            insights.append({
                'type': 'seasonality',
                'title': f"{channel} Ä‘á»‰nh thÃ¡ng {peak_month}",
                'data': f"Cáº§n chuáº©n bá»‹ trÆ°á»›c 2-3 thÃ¡ng",
                'action': f"Reserve budget & resources tá»« thÃ¡ng {peak_month - 2}"
            })

    # 4. Cross-sell Opportunities (tá»« correlation)
    # ... (náº¿u cÃ³ correlation cao)

    return insights
```

---

## ğŸ”— LIÃŠN Káº¾T Vá»šI CÃC FILE KHÃC

- **Tá»•ng quan**: [`workflow-analytics-tong-quan.md`](workflow-analytics-tong-quan.md)
- **Phases 5-8**: [`workflow-analytics-phases-5-8.md`](workflow-analytics-phases-5-8.md)
- **Error Handling**: [`workflow-analytics-error-handling.md`](workflow-analytics-error-handling.md)
- **Decision Trees**: [`workflow-analytics-decision-trees.md`](workflow-analytics-decision-trees.md)
- **Configuration**: [`workflow-analytics-configuration.md`](workflow-analytics-configuration.md)
- **Examples**: [`workflow-analytics-examples.md`](workflow-analytics-examples.md)